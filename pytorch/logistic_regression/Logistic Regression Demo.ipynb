{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:04, 2010479.38it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 118818.23it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1086183.70it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 44784.20it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "data_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/learning_datasets/'\n",
    "train_dataset = dsets.MNIST(root=data_path, \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root=data_path, \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 1000\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.8781546354293823. Accuracy: 70\n",
      "Iteration: 1000. Loss: 1.5917553901672363. Accuracy: 77\n",
      "Iteration: 1500. Loss: 1.3258112668991089. Accuracy: 80\n",
      "Iteration: 2000. Loss: 1.1923649311065674. Accuracy: 81\n",
      "Iteration: 2500. Loss: 1.1072323322296143. Accuracy: 82\n",
      "Iteration: 3000. Loss: 1.0015507936477661. Accuracy: 82\n",
      "Iteration: 3500. Loss: 0.9187393188476562. Accuracy: 83\n",
      "Iteration: 4000. Loss: 0.905571460723877. Accuracy: 83\n",
      "Iteration: 4500. Loss: 0.834633469581604. Accuracy: 84\n",
      "Iteration: 5000. Loss: 0.8056172132492065. Accuracy: 84\n",
      "Iteration: 5500. Loss: 0.7696231007575989. Accuracy: 85\n",
      "Iteration: 6000. Loss: 0.7239028215408325. Accuracy: 85\n",
      "Iteration: 6500. Loss: 0.7231870889663696. Accuracy: 85\n",
      "Iteration: 7000. Loss: 0.6778650879859924. Accuracy: 85\n",
      "Iteration: 7500. Loss: 0.6815451383590698. Accuracy: 86\n",
      "Iteration: 8000. Loss: 0.6746867299079895. Accuracy: 86\n",
      "Iteration: 8500. Loss: 0.6624687910079956. Accuracy: 86\n",
      "Iteration: 9000. Loss: 0.6437838077545166. Accuracy: 86\n",
      "Iteration: 9500. Loss: 0.6058710813522339. Accuracy: 86\n",
      "Iteration: 10000. Loss: 0.6463161110877991. Accuracy: 87\n",
      "Iteration: 10500. Loss: 0.6112169623374939. Accuracy: 87\n",
      "Iteration: 11000. Loss: 0.5632056593894958. Accuracy: 87\n",
      "Iteration: 11500. Loss: 0.5712581872940063. Accuracy: 87\n",
      "Iteration: 12000. Loss: 0.5349894762039185. Accuracy: 87\n",
      "Iteration: 12500. Loss: 0.5327935814857483. Accuracy: 87\n",
      "Iteration: 13000. Loss: 0.5670849680900574. Accuracy: 87\n",
      "Iteration: 13500. Loss: 0.5262372493743896. Accuracy: 87\n",
      "Iteration: 14000. Loss: 0.5767636895179749. Accuracy: 87\n",
      "Iteration: 14500. Loss: 0.5431764125823975. Accuracy: 87\n",
      "Iteration: 15000. Loss: 0.5265147089958191. Accuracy: 87\n",
      "Iteration: 15500. Loss: 0.5141050815582275. Accuracy: 87\n",
      "Iteration: 16000. Loss: 0.5082944631576538. Accuracy: 88\n",
      "Iteration: 16500. Loss: 0.4889393150806427. Accuracy: 88\n",
      "Iteration: 17000. Loss: 0.5228061079978943. Accuracy: 88\n",
      "Iteration: 17500. Loss: 0.5235693454742432. Accuracy: 88\n",
      "Iteration: 18000. Loss: 0.5114223957061768. Accuracy: 88\n",
      "Iteration: 18500. Loss: 0.471699595451355. Accuracy: 88\n",
      "Iteration: 19000. Loss: 0.48082098364830017. Accuracy: 88\n",
      "Iteration: 19500. Loss: 0.4797235429286957. Accuracy: 88\n",
      "Iteration: 20000. Loss: 0.4608723819255829. Accuracy: 88\n",
      "Iteration: 20500. Loss: 0.48166945576667786. Accuracy: 88\n",
      "Iteration: 21000. Loss: 0.5144409537315369. Accuracy: 88\n",
      "Iteration: 21500. Loss: 0.49219614267349243. Accuracy: 88\n",
      "Iteration: 22000. Loss: 0.48911163210868835. Accuracy: 88\n",
      "Iteration: 22500. Loss: 0.4753488302230835. Accuracy: 88\n",
      "Iteration: 23000. Loss: 0.4761657416820526. Accuracy: 88\n",
      "Iteration: 23500. Loss: 0.45139190554618835. Accuracy: 88\n",
      "Iteration: 24000. Loss: 0.4688900113105774. Accuracy: 88\n",
      "Iteration: 24500. Loss: 0.463550329208374. Accuracy: 88\n",
      "Iteration: 25000. Loss: 0.4630802869796753. Accuracy: 88\n",
      "Iteration: 25500. Loss: 0.5011271834373474. Accuracy: 88\n",
      "Iteration: 26000. Loss: 0.44763872027397156. Accuracy: 88\n",
      "Iteration: 26500. Loss: 0.4377047121524811. Accuracy: 88\n",
      "Iteration: 27000. Loss: 0.44886478781700134. Accuracy: 89\n",
      "Iteration: 27500. Loss: 0.41897904872894287. Accuracy: 89\n",
      "Iteration: 28000. Loss: 0.4445940852165222. Accuracy: 89\n",
      "Iteration: 28500. Loss: 0.46795448660850525. Accuracy: 89\n",
      "Iteration: 29000. Loss: 0.43155667185783386. Accuracy: 89\n",
      "Iteration: 29500. Loss: 0.4458855390548706. Accuracy: 89\n",
      "Iteration: 30000. Loss: 0.4569703936576843. Accuracy: 89\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, 28*28).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = Variable(images.view(-1, 28*28).cuda())\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
